# 后续实验计划

## Baseline的设计

### 单机训练

1. self learning (实现) （deprecated）

### FedAvg及其变种算法

1. FedAvg (实现)
2. FedProx (实现)

### 个性化联邦学习算法

1. FedBN (实现)
2. pFedme (实现)

### continual learning

1. **CFeD — the paper "Continual Federated Learning Based on Knowledge Distillation”**[https://doi.org/10.24963/ijcai.2022/303](https://doi.org/10.24963/ijcai.2022/303) — [https://github.com/lianziqt/CFeD](https://github.com/lianziqt/CFeD)  -- 不满足场景要求，剔除
2. FedWelT — **Federated Continual Learning with Weighted Inter-client Transfer** [[paper]](https://proceedings.mlr.press/v139/yoon21b.html?ref=https://githubhelp.com) — official but tensorflow: [https://github.com/wyjeong/FedWeIT](https://github.com/wyjeong/FedWeIT); others but pytorch: [https://github.com/luopanyaxin/-FedWEIT/tree/main/LongLifeMethod](https://github.com/luopanyaxin/-FedWEIT/tree/main/LongLifeMethod) -- 有涉及knowledge base的更新部分 -- 实现起来可以魔改，问题不大
3. FCIL —  CVPR 2022 paper ["**Federated Class-Incremental Learning**"](https://cvpr2022.thecvf.com/).  — [https://github.com/conditionWang/FCIL?tab=readme-ov-file](https://github.com/conditionWang/FCIL?tab=readme-ov-file) -- 不满足场景要求，剔除
4. FedStream — **FedStream: Prototype-Based Federated Learning on Distributed Concept-drifting Data Streams — Published in IEEE Transactions on Systems, Man, and Cybernetics: Systems** [https://ieeexplore.ieee.org/abstract/document/10198520](https://ieeexplore.ieee.org/abstract/document/10198520) — [https://github.com/mvisionai/FedStream/tree/main](https://github.com/mvisionai/FedStream/tree/main) -- 首先会在server端对用户群体进行分类（cluster），不是特别建议采用
5. (new) ICLR2023 -**BETTER GENERATIVE REPLAY FOR CONTINUAL FEDERATED LEARNING**，[[paper]](https://arxiv.org/pdf/2302.13001.pdf), [[code]](https://github.com/daiqing98/FedCIL)-Pytorch -- 不满足场景要求，剔除
6. (new) ICLR2024 -**Accurate Forgetting for Heterogeneous Federated Continual Learning**, [[paper]](https://openreview.net/pdf?id=ShQrnAsbPI), [[code]](https://github.com/zaocan666/AF-FCL)-Pytorch -- 建议这个算法，全部修改在client部分

### 注：continual可以参考这个里面的论文

Awesome Incremental Learning / Lifelong learning [[link]](https://github.com/xialeiliu/Awesome-Incremental-Learning)

### streaming data scenarios

1. DisDiff: the data distribution at the next time step is markedly different from the current time step
    - 代码里面：
    - 每一个global round中，每一个用户本地使用的训练数据都重新生成 — 这次生成的数据分布要与上次生成的数据分布完全不同/完全相反
2. HisReflection: the data distribution at the next time step may reflect the distribution of data at a specific moment in history
    - 代码里面：
    - 每一个round每个用户生成的数据分布有一定可能会重现过去的某一个时刻某个用户的数据分布

对比：

1. accuracy：test accuracy/ test loss（回归任务）
2. 使用的storage cost
3. communication cost
